{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNSDW/69GpNFcp7JAs0mtE6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 🧠 ESN-LM-garage / summer-studio\n","\n","Echo State Network（ESN）を使って、自宅やColab上で軽量な言語モデルを構築するためのスタートセット。\n","\n"],"metadata":{"id":"11qY2fg9DZKS"}},{"cell_type":"code","source":["!pip install torch numpy scikit-learn tqdm datasets sentencepiece\n"],"metadata":{"id":"R4bWtbXxDqua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🌱 概要\n","\n","このプロジェクトは、リザーバーニューラルネットワーク（ESN）を中核にした新しいタイプの言語モデルを構築することを目的とする。  \n","通常のトランスフォーマーのように大量のパラメータを学習せず、**内部リザバーは固定されたカオス的ネットワーク**として機能する。  \n","学習対象は主に「出力層（Readout）」のみであるため、高速かつ省メモリで学習可能。\n"],"metadata":{"id":"EiCgiJCCDruk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","class ESNReservoir(nn.Module):\n","    def __init__(self, in_dim, res_size=1024, spectral_radius=0.9, sparsity=0.02, leak_rate=1.0, device='cuda'):\n","        super().__init__()\n","        self.in_dim = in_dim\n","        self.res_size = res_size\n","        self.leak = leak_rate\n","        self.device = device\n","\n","        Win = torch.randn(res_size, in_dim, device=device) * 0.5\n","        W = torch.randn(res_size, res_size, device=device)\n","        mask = (torch.rand_like(W) < sparsity).float()\n","        W = W * mask\n","\n","        with torch.no_grad():\n","            eigvals = torch.linalg.eigvals(W)\n","            max_ev = max(abs(eigvals).real.max().item(), 1e-6)\n","            W = W * (spectral_radius / max_ev)\n","\n","        self.register_buffer('Win', Win)\n","        self.register_buffer('W', W)\n","        self.state = torch.zeros(1, res_size, device=device)\n","\n","    def forward(self, u):\n","        pre = F.linear(u, self.Win) + F.linear(self.state, self.W)\n","        new_state = (1 - self.leak) * self.state + self.leak * torch.tanh(pre)\n","        self.state = new_state\n","        return new_state\n","\n","    def reset_state(self, batch_size=1):\n","        self.state = torch.zeros(batch_size, self.res_size, device=self.device)\n","\n","class ESNLanguageModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=128, res_size=1024, device='cuda'):\n","        super().__init__()\n","        self.device = device\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.reservoir = ESNReservoir(embed_dim, res_size, device=device)\n","        self.readout = nn.Linear(res_size, vocab_size)\n","\n","    def forward(self, input_ids):\n","        self.reservoir.reset_state(batch_size=input_ids.size(0))\n","        outputs = []\n","        for t in range(input_ids.size(1)):\n","            x = self.embedding(input_ids[:, t])\n","            res_state = self.reservoir(x)\n","            logits = self.readout(res_state)\n","            outputs.append(logits.unsqueeze(1))\n","        return torch.cat(outputs, dim=1)\n"],"metadata":{"id":"hNdxwlEtDsM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import torch\n","\n","dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")  # 1%だけ使う\n","text_data = \"\\n\".join(dataset[\"text\"])\n","\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","tokens = tokenizer(text_data, return_tensors=\"pt\", truncation=True, max_length=256)\n","input_ids = tokens[\"input_ids\"]\n","\n","# シーケンスをバッチ分割\n","seq_len = 64\n","inputs = []\n","targets = []\n","for i in range(0, input_ids.size(1) - seq_len - 1, seq_len):\n","    inputs.append(input_ids[:, i:i+seq_len])\n","    targets.append(input_ids[:, i+1:i+seq_len+1])\n","\n","inputs = torch.cat(inputs)\n","targets = torch.cat(targets)\n","print(f\"Dataset size: {inputs.size()}\")\n"],"metadata":{"id":"xuSuYJmZDvYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","vocab_size = tokenizer.vocab_size\n","\n","model = ESNLanguageModel(vocab_size, embed_dim=128, res_size=512, device=device).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n","\n","epochs = 120\n","batch_size = 8\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    for i in tqdm(range(0, inputs.size(0), batch_size)):\n","        batch_in = inputs[i:i+batch_size].to(device)\n","        batch_tg = targets[i:i+batch_size].to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(batch_in)\n","        loss = F.cross_entropy(logits.view(-1, vocab_size), batch_tg.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}: Loss = {total_loss / max(1, (inputs.size(0)//batch_size)):.4f}\")\n","\n"],"metadata":{"id":"4PqfBGGXDztg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(model, tokenizer, prompt=\"The universe\", max_new_tokens=50):\n","    model.eval()\n","    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n","    model.reservoir.reset_state(1)\n","\n","    for _ in range(max_new_tokens):\n","        logits = model(tokens)\n","        next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n","        tokens = torch.cat([tokens, next_token], dim=1)\n","    text = tokenizer.decode(tokens[0])\n","    return text\n","\n","print(generate_text(model, tokenizer, prompt=\"Artificial intelligence\"))\n"],"metadata":{"id":"Vgf8qHWsD3D8"},"execution_count":null,"outputs":[]}]}